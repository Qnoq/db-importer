# SQL Data Importer ðŸš€

A secure, production-ready web application for importing data from Excel/CSV files into SQL databases. This tool helps you:
- Parse SQL schema dumps (MySQL/MariaDB and PostgreSQL)
- Upload Excel or CSV data files
- Automatically map columns with intelligent matching (Levenshtein distance algorithm)
- Validate data against schema constraints
- Generate type-safe SQL INSERT statements

**No database connection required** - You execute the generated SQL yourself in your own environment.

## ðŸŒŸ Key Features

### Security & Robustness
- âœ… **Type-aware SQL generation** - Proper formatting for numbers, strings, booleans, dates
- âœ… **SQL injection prevention** - Comprehensive escaping and validation
- âœ… **Rate limiting** - Protection against abuse (configurable)
- âœ… **Configurable CORS** - No more wildcards in production
- âœ… **Input validation** - File size, type, and data validation
- âœ… **Structured logging** - JSON logs with multiple levels (DEBUG, INFO, WARN, ERROR)
- âœ… **Error handling** - Clear, actionable error messages

### Data Processing
- âœ… **Smart column mapping** - Levenshtein distance algorithm for fuzzy matching
- âœ… **Data type validation** - Server-side validation against schema
- âœ… **NOT NULL checks** - Prevents missing required fields
- âœ… **Type length validation** - VARCHAR/CHAR length checks
- âœ… **Range validation** - Numeric type range checks (TINYINT, SMALLINT, etc.)
- âœ… **State persistence** - LocalStorage integration (no data loss on refresh)

### Developer Experience
- âœ… **TypeScript** - Strict typing throughout frontend
- âœ… **Multi-stage Docker builds** - Optimized images (10x smaller)
- âœ… **Health checks** - Built-in monitoring
- âœ… **Environment variables** - Fully configurable
- âœ… **Non-root containers** - Security best practices
- âœ… **Hot reload** - Fast development cycle

## Tech Stack

### Backend
- **Language**: Go 1.21
- **Framework**: Standard library `net/http`
- **Architecture**: Clean architecture with layers
  - Config management
  - Structured logging
  - Error handling
  - Middleware (CORS, rate limiting, logging)
  - SQL parsing (Vitess + regex fallback)
  - Type-aware SQL generation

### Frontend
- **Framework**: Vue 3 (Composition API)
- **Build tool**: Vite
- **UI Components**: PrimeVue
- **Styling**: TailwindCSS
- **State management**: Pinia with localStorage persistence
- **Data parsing**: SheetJS (Excel)
- **Language**: TypeScript (strict mode)

## Quick Start

### Prerequisites
- Docker & Docker Compose
- (Optional) Go 1.21+ and Node.js 20+ for local development

### Development Mode

1. Clone the repository
   ```bash
   git clone <your-repo-url>
   cd db-importer
   ```

2. Start development environment
   ```bash
   docker-compose -f docker-compose.dev.yml up
   ```

3. Access the application
   - Frontend: http://localhost:5173
   - Backend API: http://localhost:8080

### Production Mode

1. Copy environment template
   ```bash
   cp .env.example .env
   ```

2. Configure environment variables
   ```bash
   # Edit .env file
   ALLOWED_ORIGINS=https://yourdomain.com
   PORT=8080
   MAX_UPLOAD_SIZE=52428800
   RATE_LIMIT_ENABLED=true
   ```

3. Start production environment
   ```bash
   docker-compose up -d
   ```

4. Access the application
   - Frontend: http://localhost:8081
   - Backend API: http://localhost:8080

## ðŸ“š API Documentation

Interactive API documentation is available via **Swagger/OpenAPI**:

- **Swagger UI**: http://localhost:3000/swagger/
- **OpenAPI Spec (JSON)**: http://localhost:3000/swagger/doc.json
- **OpenAPI Spec (YAML)**: http://localhost:3000/swagger/doc.yaml

The Swagger UI provides:
- âœ… Complete API reference for all endpoints
- âœ… Interactive "Try it out" feature to test endpoints
- âœ… Request/response schemas with examples
- âœ… Authentication flows (Bearer token for protected endpoints)
- âœ… Organized by tags (Health, Schema, SQL, Auth, Imports)

### Regenerating Documentation

If you modify API annotations, regenerate the Swagger docs:

```bash
cd backend
swag init -g cmd/server/docs.go -o docs --parseDependency --parseInternal
```

The generated documentation files (`docs/`) are committed to the repository, so the Swagger UI works out of the box.

## Usage Guide

### Step 1: Upload SQL Schema
Upload your database dump file (.sql) containing CREATE TABLE statements. The parser supports:
- MySQL/MariaDB syntax
- PostgreSQL syntax
- Complex types (VARCHAR, INT, DATETIME, BOOLEAN, etc.)
- Constraints (NOT NULL, defaults, etc.)

### Step 2: Select Target Table
Choose the table where you want to import data. You'll see:
- Number of columns
- Column names preview
- Table metadata

### Step 3: Upload Data File
Upload an Excel (.xlsx, .xls) or CSV file with your data. Features:
- Preview of first 10 rows
- Total row count
- Header detection
- Encoding support

### Step 4: Map Columns
**Automatic mapping** uses Levenshtein distance for intelligent matching:
- Exact matches (100% confidence)
- Fuzzy matches (60%+ similarity)
- Manual override available

**Validation warnings** show:
- Unmapped NOT NULL fields
- Missing required columns
- Type mismatches

### Step 5: Generate & Download SQL
Click "Generate & Download SQL" to:
- Validate all data against schema
- Format values by type (no more quoted numbers!)
- Generate optimized INSERT statements
- Download ready-to-execute SQL file

### Step 6: Execute SQL
Run the generated SQL file in your database environment:
```bash
mysql -u user -p database < import_customers_1234567890.sql
# or
psql -U user -d database -f import_customers_1234567890.sql
```

## API Endpoints

### POST /parse-schema
Parse a SQL dump file and extract table structures.

**Request**: Multipart form with SQL file

**Response**:
```json
{
  "tables": [
    {
      "name": "customers",
      "fields": [
        { "name": "id", "type": "int", "nullable": false },
        { "name": "email", "type": "varchar(255)", "nullable": false },
        { "name": "age", "type": "tinyint", "nullable": true }
      ]
    }
  ]
}
```

### POST /generate-sql
Generate INSERT statements from mapped data with validation.

**Request**:
```json
{
  "table": "customers",
  "mapping": { "Email": "email", "Age": "age" },
  "rows": [
    ["john@test.com", "25"],
    ["sarah@test.com", "30"]
  ],
  "fields": [
    { "name": "email", "type": "varchar(255)", "nullable": false },
    { "name": "age", "type": "tinyint", "nullable": true }
  ]
}
```

**Response** (HTTP 200):
```sql
INSERT INTO `customers` (`email`, `age`) VALUES
('john@test.com', 25),
('sarah@test.com', 30);
```

**Error Response** (HTTP 422):
```json
{
  "error": "Data validation failed",
  "detail": "Some data does not match field constraints",
  "errors": [
    "Row 3, Field 'age': value 200 out of range for TINYINT"
  ]
}
```

### GET /health
Health check endpoint with config info.

**Response**:
```json
{
  "status": "ok",
  "version": "1.0.0",
  "config": {
    "maxUploadSize": 52428800,
    "rateLimitEnabled": true
  }
}
```

## Configuration

All configuration via environment variables:

| Variable | Default | Description |
|----------|---------|-------------|
| `PORT` | `8080` | Backend server port |
| `ALLOWED_ORIGINS` | `*` | CORS allowed origins (comma-separated) |
| `MAX_UPLOAD_SIZE` | `52428800` | Max upload size in bytes (50MB) |
| `DEBUG_LOG` | `false` | Enable debug logging |
| `RATE_LIMIT_ENABLED` | `true` | Enable rate limiting |
| `RATE_LIMIT_REQUESTS` | `100` | Max requests per window |
| `RATE_LIMIT_WINDOW` | `60` | Rate limit window (seconds) |
| `VITE_API_URL` | `http://localhost:8080` | Frontend API URL |

## Project Structure

```
db-importer/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ config/           # Configuration management
â”‚   â”œâ”€â”€ errors/           # Error types and handlers
â”‚   â”œâ”€â”€ generator/        # Type-aware SQL generation
â”‚   â”œâ”€â”€ logger/           # Structured JSON logging
â”‚   â”œâ”€â”€ middleware/       # HTTP middleware
â”‚   â”œâ”€â”€ parser/           # SQL schema parsers
â”‚   â”œâ”€â”€ main.go           # Main application
â”‚   â”œâ”€â”€ Dockerfile        # Production image (multi-stage)
â”‚   â””â”€â”€ go.mod
â”‚
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ pages/        # Vue page components
â”‚   â”‚   â”œâ”€â”€ router/       # Vue Router
â”‚   â”‚   â”œâ”€â”€ store/        # Pinia state (with localStorage)
â”‚   â”‚   â”œâ”€â”€ App.vue
â”‚   â”‚   â””â”€â”€ main.ts
â”‚   â”œâ”€â”€ Dockerfile        # Production image (nginx)
â”‚   â”œâ”€â”€ Dockerfile.dev    # Development image
â”‚   â”œâ”€â”€ nginx.conf        # Nginx configuration
â”‚   â””â”€â”€ package.json
â”‚
â”œâ”€â”€ examples/             # Test files
â”œâ”€â”€ docker-compose.yml    # Production compose
â”œâ”€â”€ docker-compose.dev.yml # Development compose
â”œâ”€â”€ .env.example          # Environment template
â”œâ”€â”€ SECURITY.md           # Security documentation
â”œâ”€â”€ CONTRIBUTING.md       # Contribution guide
â””â”€â”€ README.md
```

## Development

### Backend

```bash
cd backend
go mod download
go run main.go
```

### Frontend

```bash
cd frontend
npm install
npm run dev
```

### Building

```bash
# Backend
cd backend
go build -o db-importer main.go

# Frontend
cd frontend
npm run build
```

## Security

This application implements multiple security layers:

- **No SQL injection**: Type-aware generation with proper escaping
- **Rate limiting**: Configurable per-IP limits
- **Input validation**: File type, size, and data validation
- **CORS protection**: Configurable origins
- **No database access**: Zero-trust architecture
- **Non-root containers**: Docker security best practices
- **Structured logging**: No sensitive data in logs

See [SECURITY.md](SECURITY.md) for detailed security information.

## Contributing

We welcome contributions! See [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

## Examples

Test files provided in [`examples/`](examples/):

| File | Description |
|------|-------------|
| `mysql_customers.sql` | MySQL schema with customers, orders, products |
| `postgres_users.sql` | PostgreSQL schema with users, posts, comments |
| `customers_data.csv` | Sample customer data |
| `products_data.csv` | Sample product data |

## Troubleshooting

### Parser finds no tables
- Ensure SQL file contains valid `CREATE TABLE` statements
- Check for syntax errors in SQL
- Try both MySQL and PostgreSQL formats

### Frontend can't connect to backend
- Verify `VITE_API_URL` is set correctly
- Check CORS configuration
- Ensure backend is running

### Rate limit errors
- Adjust `RATE_LIMIT_REQUESTS` and `RATE_LIMIT_WINDOW`
- Or disable with `RATE_LIMIT_ENABLED=false` (dev only)

### Upload size errors
- Increase `MAX_UPLOAD_SIZE`
- Check available disk space

## Performance

- **Parsing**: Handles schemas with 100+ tables instantly
- **Data processing**: Processes 10,000+ rows in < 1 second
- **Memory usage**: < 50MB for typical workloads
- **Docker images**:
  - Backend: ~20MB (multi-stage build)
  - Frontend: ~30MB (nginx + static files)

## License

MIT

## Support

- **Issues**: Open an issue on GitHub
- **Security**: See [SECURITY.md](SECURITY.md)
- **Contributing**: See [CONTRIBUTING.md](CONTRIBUTING.md)

---

**Made with â¤ï¸ for developers who need to import data safely**
